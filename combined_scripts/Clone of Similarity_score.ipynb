{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "!pip install pyarrow\n",
    "!pip install wrds"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Collecting pyarrow\r\n",
      "  Downloading pyarrow-15.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (38.4 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0\/38.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9\/38.4 MB\u001b[0m \u001b[31m55.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3\/38.4 MB\u001b[0m \u001b[31m47.2 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0\/38.4 MB\u001b[0m \u001b[31m47.9 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6\/38.4 MB\u001b[0m \u001b[31m46.9 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2\/38.4 MB\u001b[0m \u001b[31m46.5 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9\/38.4 MB\u001b[0m \u001b[31m49.7 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8\/38.4 MB\u001b[0m \u001b[31m59.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6\/38.4 MB\u001b[0m \u001b[31m54.7 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7\/38.4 MB\u001b[0m \u001b[31m54.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3\/38.4 MB\u001b[0m \u001b[31m62.7 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4\/38.4 MB\u001b[0m \u001b[31m53.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6\/38.4 MB\u001b[0m \u001b[31m50.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3\/38.4 MB\u001b[0m \u001b[31m52.2 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m27.7\/38.4 MB\u001b[0m \u001b[31m53.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m30.4\/38.4 MB\u001b[0m \u001b[31m60.5 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m32.0\/38.4 MB\u001b[0m \u001b[31m56.7 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m33.6\/38.4 MB\u001b[0m \u001b[31m54.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m34.6\/38.4 MB\u001b[0m \u001b[31m46.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m35.7\/38.4 MB\u001b[0m \u001b[31m44.4 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m37.3\/38.4 MB\u001b[0m \u001b[31m41.5 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m40.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m40.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m40.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m40.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m40.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m19.9 MB\/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16.6 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from pyarrow) (1.24.3)\r\n",
      "Installing collected packages: pyarrow\r\n",
      "Successfully installed pyarrow-15.0.0\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Collecting wrds\r\n",
      "  Downloading wrds-3.2.0-py3-none-any.whl (13 kB)\r\n",
      "INFO: pip is looking at multiple versions of wrds to determine which version is compatible with other requirements. This could take a while.\r\n",
      "  Downloading wrds-3.1.6-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: numpy in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wrds) (1.24.3)\r\n",
      "Requirement already satisfied: pandas in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wrds) (1.5.3)\r\n",
      "Requirement already satisfied: psycopg2-binary in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wrds) (2.9.6)\r\n",
      "Requirement already satisfied: scipy in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wrds) (1.9.1)\r\n",
      "Collecting sqlalchemy<2 (from wrds)\r\n",
      "  Downloading SQLAlchemy-1.4.51-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0\/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6\/1.6 MB\u001b[0m \u001b[31m69.5 MB\/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from sqlalchemy<2->wrds) (3.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from pandas->wrds) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from pandas->wrds) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from python-dateutil>=2.8.1->pandas->wrds) (1.16.0)\r\n",
      "Installing collected packages: sqlalchemy, wrds\r\n",
      "  Attempting uninstall: sqlalchemy\r\n",
      "    Found existing installation: SQLAlchemy 2.0.13\r\n",
      "    Uninstalling SQLAlchemy-2.0.13:\r\n",
      "      Successfully uninstalled SQLAlchemy-2.0.13\r\n",
      "Successfully installed sqlalchemy-1.4.51 wrds-3.1.6\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"fuUDYtXG961djHrK6SHaxM",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "import pickle\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import wrds\n",
    "import pytz\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import winsorize"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"VSOYDZa9rHC3nyYHaV3MmX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "folder_path = r'\/data\/workspace_files\/parquet_files'\n",
    "files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"rTjTnRVdL8y7fIK5ZGn5Ax",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "files"
   ],
   "execution_count":5,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "['New file']"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"md2HCMmJWXZXtyWaWk5VyY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    try:\n",
    "        dict1 = ast.literal_eval(str1)\n",
    "        dict2 = ast.literal_eval(str2)\n",
    "        s1 = set(dict1.keys())\n",
    "        s2 = set(dict2.keys())\n",
    "        return len(s1.intersection(s2)) \/ len(s1.union(s2))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def cosine_similarity(str1, str2):\n",
    "    try:\n",
    "        dict1 = ast.literal_eval(str1)\n",
    "        dict2 = ast.literal_eval(str2)\n",
    "        list1 = [dict1.get(key, 0) for key in set(dict1.keys()) | set(dict2.keys())]\n",
    "        list2 = [dict2.get(key, 0) for key in set(dict1.keys()) | set(dict2.keys())]\n",
    "        a = np.array(list1)\n",
    "        b = np.array(list2)\n",
    "        cos_sim = np.dot(a, b) \/ (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        return cos_sim\n",
    "    except:\n",
    "        return -1"
   ],
   "execution_count":6,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"DXs10i0WKpjwWEUsBFhnyN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Specify the path to the Parquet file\n",
    "parquet_file_path = f'{folder_path}\/file_2015.parquet'\n",
    "\n",
    "# Read the Parquet file into a PyArrow Table\n",
    "table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Convert the PyArrow Table to a Pandas DataFrame (optional)\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Now, you can work with the PyArrow Table or Pandas DataFrame as needed\n",
    "df.head()"
   ],
   "execution_count":7,
   "outputs":[
    {
     "ename":"FileNotFoundError",
     "evalue":"FileNotFoundError: \/data\/workspace_files\/parquet_files\/file_2015.parquet",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 7 in <module>",
      "    at line 1776 in read_table(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)",
      "    at line 1354 in __init__(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)",
      "    at line 782 in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)",
      "    at line 465 in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)",
      "    at line 441 in _ensure_single_source(path, filesystem)",
      "FileNotFoundError: \/data\/workspace_files\/parquet_files\/file_2015.parquet"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"FJekaxG2xQVWeyzH6WLxSJ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_save(year):\n",
    "    folder_path = r'\/data\/workspace_files\/parquet_files'\n",
    "    curr_file_path = f'{folder_path}\/file_{str(year)}.parquet'\n",
    "    prev_file_path = f'{folder_path}\/file_{str(year-1)}.parquet'\n",
    "    \n",
    "    # Read only relevant columns from current Parquet file\n",
    "    table_curr = pq.read_table(curr_file_path, columns=['cik', 'form_type', 'filing_date', 'conformed_period_report', 'doc_dict_text'])\n",
    "    curr_df = table_curr.to_pandas()\n",
    "    curr_df['conformed_period_report'] = pd.to_datetime(curr_df['conformed_period_report'])\n",
    "    curr_df['month'] = curr_df['conformed_period_report'].dt.month\n",
    "    \n",
    "    # Read only relevant columns from previous Parquet file\n",
    "    table_prev = pq.read_table(prev_file_path, columns=['cik', 'form_type', 'conformed_period_report', 'doc_dict_text'])\n",
    "    prev_df = table_prev.to_pandas()\n",
    "    prev_df['conformed_period_report'] = pd.to_datetime(prev_df['conformed_period_report'])\n",
    "    prev_df['month'] = prev_df['conformed_period_report'].dt.month\n",
    "    \n",
    "    # Merge DataFrames on 'cik', 'form_type', and 'month'\n",
    "    df = pd.merge(curr_df, prev_df, on=['cik', 'form_type', 'month'], suffixes=['', '_prev'])\n",
    "    \n",
    "    df['cosine_similarity'] = df.apply(lambda x: cosine_similarity(x['doc_dict_text'], x['doc_dict_text_prev']), axis=1)\n",
    "    df['jaccard_similarity'] = df.apply(lambda x: jaccard_similarity(x['doc_dict_text'], x['doc_dict_text_prev']), axis=1)\n",
    "    \n",
    "    # Select and save only necessary columns\n",
    "    df = df[['cik', 'filing_date', 'cosine_similarity', 'jaccard_similarity']]\n",
    "    df.to_parquet(f'\/data\/workspace_files\/parquet_files\/similarity_scores\/by_conformed_period\/{str(year)}.parquet')\n",
    "    # Release memory\n",
    "    del curr_df, prev_df, table_curr, table_prev, df\n",
    "\n",
    "\n",
    "dfs = []\n"
   ],
   "execution_count":8,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"nd4wNjel8ULpb6lRx2IEo3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for year in range(2015, 2022):\n",
    "    process_and_save(year)\n",
    "    print(f'Finished: {year}')\n",
    "\n",
    "\"\"\"final_df = pd.concat(dfs)\n",
    "print(len(final_df))\n",
    "print(len(final_df['cik'].unique()))\n",
    "\n",
    "final_df['filing_monthyr'] = final_df['filing_date'].dt.strftime('%b%Y')\"\"\""
   ],
   "execution_count":7,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Finished: 2000\n",
      "Finished: 2001\n",
      "Finished: 2002\n",
      "Finished: 2003\n",
      "Finished: 2004\n",
      "Finished: 2005\n",
      "Finished: 2006\n",
      "Finished: 2007\n",
      "Finished: 2008\n",
      "Finished: 2009\n",
      "Finished: 2010\n",
      "Finished: 2011\n",
      "Finished: 2012\n",
      "Finished: 2013\n",
      "Finished: 2014\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "<ipython-input-5-580382473648>:22: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cos_sim = np.dot(a, b) \/ (np.linalg.norm(a) * np.linalg.norm(b))\n",
      "<ipython-input-5-580382473648>:22: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cos_sim = np.dot(a, b) \/ (np.linalg.norm(a) * np.linalg.norm(b))\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "\"final_df = pd.concat(dfs)\\nprint(len(final_df))\\nprint(len(final_df['cik'].unique()))\\n\\nfinal_df['filing_date'] = pd.to_datetime(final_df['filing_date'])\\nfinal_df['filing_monthyr'] = final_df['filing_date'].dt.strftime('%b%Y')\""
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"KrO0wZ3blir08ZgGETtux4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "dfs = []\n",
    "for yr in range(1996,2019):\n",
    "    df = pq.read_table(f'\/data\/workspace_files\/parquet_files\/similarity_scores\/by_conformed_period\/{str(yr)}.parquet').to_pandas()\n",
    "    dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs)\n",
    "print(len(final_df))\n",
    "print(len(final_df['cik'].unique()))\n",
    "\n",
    "final_df['filing_date'] = pd.to_datetime(final_df['filing_date'])\n",
    "final_df['filing_monthyr'] = final_df['filing_date'].dt.strftime('%b%Y')"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "624190\n",
      "25544\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"amDg9YCjcg2GoaTbRdFwbU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df"
   ],
   "execution_count":10,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>cik<\/th>\n",
       "      <th>filing_date<\/th>\n",
       "      <th>cosine_similarity<\/th>\n",
       "      <th>jaccard_similarity<\/th>\n",
       "      <th>filing_monthyr<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>25354<\/td>\n",
       "      <td>2007-06-21<\/td>\n",
       "      <td>0.770233<\/td>\n",
       "      <td>0.179398<\/td>\n",
       "      <td>Jun2007<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>102710<\/td>\n",
       "      <td>1996-07-11<\/td>\n",
       "      <td>0.016747<\/td>\n",
       "      <td>0.018256<\/td>\n",
       "      <td>Jul1996<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>104207<\/td>\n",
       "      <td>1996-07-11<\/td>\n",
       "      <td>0.989288<\/td>\n",
       "      <td>0.614891<\/td>\n",
       "      <td>Jul1996<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>318251<\/td>\n",
       "      <td>1996-07-11<\/td>\n",
       "      <td>0.969808<\/td>\n",
       "      <td>0.735027<\/td>\n",
       "      <td>Jul1996<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>34136<\/td>\n",
       "      <td>1996-07-11<\/td>\n",
       "      <td>0.985219<\/td>\n",
       "      <td>0.752542<\/td>\n",
       "      <td>Jul1996<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>29501<\/th>\n",
       "      <td>1082733<\/td>\n",
       "      <td>2018-04-05<\/td>\n",
       "      <td>0.996582<\/td>\n",
       "      <td>0.919569<\/td>\n",
       "      <td>Apr2018<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>29502<\/th>\n",
       "      <td>1041588<\/td>\n",
       "      <td>2018-06-25<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>Jun2018<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>29503<\/th>\n",
       "      <td>1041588<\/td>\n",
       "      <td>2018-06-25<\/td>\n",
       "      <td>0.993256<\/td>\n",
       "      <td>0.931889<\/td>\n",
       "      <td>Jun2018<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>29504<\/th>\n",
       "      <td>1041588<\/td>\n",
       "      <td>2018-06-25<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>Jun2018<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>29505<\/th>\n",
       "      <td>1041588<\/td>\n",
       "      <td>2018-06-25<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>1.000000<\/td>\n",
       "      <td>Jun2018<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>519966 rows × 5 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"AVoT2g7Wn1x9F0FPjiyYun",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "month_yrs = final_df['filing_monthyr'].unique()\n",
    "for month_yr in month_yrs:\n",
    "    df_filt = final_df[final_df['filing_monthyr']==month_yr]\n",
    "    df_filt.to_parquet(f'\/data\/workspace_files\/parquet_files\/similarity_scores\/{month_yr}.parquet')"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"RVvtl5fMfwJ4ebyWSoFulE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "### log-in using your credentials\n",
    "if 'conn' in locals():\n",
    "    print(\"WRDS connection already open!\")\n",
    "else:\n",
    "    conn = wrds.Connection()"
   ],
   "execution_count":9,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Enter your WRDS username [datalore]: anjali167\n",
      "Enter your password: ················\n",
      "WRDS recommends setting up a .pgpass file.\n",
      "Create .pgpass file now [y\/n]?:  y\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"WNhZYrr8Jh839U981UAYtQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_sd_and_ed_for_returns_old(date, lead):\n",
    "    start_current_month = date.replace(day=1)\n",
    "    start_current_month = start_current_month + relativedelta(months = lead)\n",
    "    start_next_month = start_current_month + relativedelta(months=1)\n",
    "    #start_current_month = start_current_month + relativedelta(months=-2)\n",
    "\n",
    "    start_current_month_str = start_current_month.strftime(\"%Y-%m-%d\")\n",
    "    start_next_month_str = start_next_month.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return start_current_month_str, start_next_month_str"
   ],
   "execution_count":12,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"BygMbszcbtRuuccR0cdb8c",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_req_tick_cik_map(start_date, end_date):\n",
    "    # Assuming you have already imported the necessary libraries and connected to your database\n",
    "    # Assuming you have already imported the necessary libraries and connected to your database\n",
    "\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT distinct cik, tic, cusip, cshtrm, trt1m\n",
    "    FROM comp_na_daily_all.secm\n",
    "    WHERE datadate >= '{start_date}' and datadate <= '{end_date}'\n",
    "    \"\"\"\n",
    "\n",
    "    cik_tick_map = conn.raw_sql(sql_query)\n",
    "    cik_tick_map = cik_tick_map.dropna(subset=['cik', 'trt1m'])\n",
    "    cik_tick_map['cik'] = cik_tick_map['cik'].astype(int)\n",
    "\n",
    "    print(\"Total rows:\", len(cik_tick_map))\n",
    "    print(\"Unique cusip count:\", cik_tick_map['cusip'].nunique())\n",
    "\n",
    "    # Identify 'cik' values with multiple rows\n",
    "    cik_counts = cik_tick_map['cik'].value_counts()\n",
    "    multiple_cik_values = cik_counts[cik_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a mask to filter rows for 'cik' values with multiple rows\n",
    "    multiple_cik_mask = cik_tick_map['cik'].isin(multiple_cik_values)\n",
    "\n",
    "    # Create a mask to identify rows with null 'cshtrm'\n",
    "    null_cshtrm_mask = cik_tick_map['cshtrm'].isnull()\n",
    "\n",
    "    # Apply both masks to filter rows for 'cik' values with multiple rows\n",
    "    filtered_cik_tick_map = cik_tick_map[multiple_cik_mask & ~null_cshtrm_mask]\n",
    "\n",
    "    # Group by 'cik' and apply idxmax() to find the index with the largest 'cshtrm'\n",
    "    grouped_map = filtered_cik_tick_map.groupby(['cik'])['cshtrm'].idxmax()\n",
    "\n",
    "    # Use loc to select the rows with the largest 'cshtrm' for 'cik' values with multiple rows\n",
    "    cik_tick_map_with_max_cshtrm = filtered_cik_tick_map.loc[grouped_map]\n",
    "\n",
    "\n",
    "    # Select rows for 'cik' values that have only one row\n",
    "    single_row_cik_values = cik_counts[cik_counts == 1].index.tolist()\n",
    "    single_row_cik_mask = cik_tick_map['cik'].isin(single_row_cik_values)\n",
    "    cik_tick_map_single_rows = cik_tick_map[single_row_cik_mask]\n",
    "\n",
    "    # Append rows for 'cik' values with one row to the previously selected rows\n",
    "    final_cik_tick_map = pd.concat([cik_tick_map_with_max_cshtrm, cik_tick_map_single_rows], axis = 0)\n",
    "    # Rename the 'trt1m' column to 'ret'\n",
    "    final_cik_tick_map.rename(columns={'trt1m': 'ret'}, inplace=True)\n",
    "    \n",
    "    print(\"Total rows after selecting rows with max cshtrm:\", len(final_cik_tick_map))\n",
    "\n",
    "\n",
    "    return final_cik_tick_map"
   ],
   "execution_count":13,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"uoEAvtjITQHkFUdPqIqZT1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def create_directories():\n",
    "    folder_path = '\/data\/workspace_files\/parquet_files'\n",
    "    for score_type in ['cosine_similarity', 'jaccard_similarity']:\n",
    "            for lead in range(1,13):\n",
    "                folder_path = f'{folder_path}\/portfolio_lead_lag\/{score_type}\/{lead}\n",
    "                print(folder_path)\n",
    "                os.makedirs(folder_path, exist_ok = True)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lqdWnB4EjO8Lcn6mlXuw1H",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "month_yr = 'Jan1996'\n",
    "date_obj = datetime.strptime(month_yr, '%b%Y')\n",
    "sd_for_cik_ticker_map, ed_for_cik_ticker_map = get_sd_and_ed_for_returns_old(date_obj)\n",
    "print(sd_for_cik_ticker_map, ed_for_cik_ticker_map)\n",
    "cik_tick_map_req = get_req_tick_cik_map(start_date=sd_for_cik_ticker_map, end_date=ed_for_cik_ticker_map)\n",
    "score_type = 'jaccard_similarity'\n",
    "score_path = f'{folder_path}\/similarity_scores\/{month_yr}.parquet'\n",
    "scores = pq.read_table(score_path).to_pandas()\n",
    "cik_tick_map_req['ret_clipped'] = cik_tick_map_req['ret'].clip(lower=-50.0, upper=50.0)\n",
    "scores = pd.merge(scores, cik_tick_map_req, how = 'inner', left_on = ['cik'], right_on = ['cik'])\n",
    "print(len(scores))\n",
    "\n",
    "# Extract the similarity scores\n",
    "similarity_scores = scores[score_type]\n",
    "\n",
    "# Create buckets (quantiles) for long and short positions\n",
    "num_buckets = 5  # Number of quantiles, you can adjust this\n",
    "buckets = pd.qcut(similarity_scores, q=num_buckets, labels=False, duplicates='drop')\n",
    "\n",
    "# Assign long and short positions based on buckets\n",
    "long_positions = scores[buckets == (num_buckets - 1)]\n",
    "short_positions = scores[buckets == 0]\n",
    "long_positions['pos_type'] = 1\n",
    "short_positions['pos_type'] = -1\n",
    "long_positions['weight'] = long_positions['cshtrm'] \/ long_positions['cshtrm'].sum()\n",
    "short_positions['weight'] = -1*short_positions['cshtrm'] \/ short_positions['cshtrm'].sum()\n",
    "\n",
    "\n",
    "positions = pd.concat([long_positions, short_positions])\n",
    "positions['num_months'] = 1\n",
    "positions['date'] = month_yr\n",
    "positions = positions[['date', 'cik', 'pos_type', 'num_months', 'weight', 'ret', 'ret_clipped', 'filing_date', 'cshtrm']]\n",
    "positions.to_parquet(f'\/data\/workspace_files\/parquet_files\/portfolios2\/{score_type}\/{month_yr}.parquet')\n",
    "\n",
    "print(f'Num Holdings: {len(positions)}')\n",
    "    "
   ],
   "execution_count":46,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "1996-02-01 1996-03-01\n",
      "1996-02-01\n",
      "1996-03-01\n",
      "Total rows: 10368\n",
      "Unique cusip count: 10176\n",
      "Total rows after selecting rows with max cshtrm: 9975\n",
      "2\n",
      "Num Holdings: 2\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "<ipython-input-46-01ec7d437913>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_positions['pos_type'] = 1\n",
      "<ipython-input-46-01ec7d437913>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_positions['pos_type'] = -1\n",
      "<ipython-input-46-01ec7d437913>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_positions['weight'] = long_positions['cshtrm'] \/ long_positions['cshtrm'].sum()\n",
      "<ipython-input-46-01ec7d437913>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_positions['weight'] = -1*short_positions['cshtrm'] \/ short_positions['cshtrm'].sum()\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6XZ4kEnh8lEHDTRBE5CO82",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "folder_path = '\/data\/workspace_files\/parquet_files'\n",
    "month_dict = { 1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "# Loop from March 2015 to March 2021\n",
    "for score_type in ['cosine_similarity', 'jaccard_similarity']:\n",
    "    for year in range(1996, 2018):\n",
    "        for month in range(1, 13):\n",
    "            for lead in range(1,13):\n",
    "                if year == 1996 and month <= 1:\n",
    "                    continue\n",
    "                month_yr = f\"{month_dict[month]}{year}\"\n",
    "                # Load similarity scores\n",
    "                score_path = f'{folder_path}\/similarity_scores\/{month_yr}.parquet'\n",
    "                scores = pq.read_table(score_path).to_pandas()\n",
    "                print(len(scores))\n",
    "                date_obj = datetime.strptime(month_yr, '%b%Y')\n",
    "                sd_for_cik_ticker_map, ed_for_cik_ticker_map = get_sd_and_ed_for_returns_old(date_obj, lead)\n",
    "                cik_tick_map_req = get_req_tick_cik_map(start_date=sd_for_cik_ticker_map, end_date=ed_for_cik_ticker_map)\n",
    "                cik_tick_map_req['ret_clipped'] = cik_tick_map_req['ret'].clip(lower=-50.0, upper=50.0)\n",
    "                scores = pd.merge(scores, cik_tick_map_req, how = 'inner', left_on = ['cik'], right_on = ['cik'])\n",
    "                print(len(scores))\n",
    "\n",
    "                # Extract similarity scores\n",
    "                similarity_scores = scores[score_type]\n",
    "\n",
    "                # Create buckets (quantiles) for long and short positions\n",
    "                num_buckets = 5  # Number of quantiles, you can adjust this\n",
    "                buckets = pd.qcut(similarity_scores, q=num_buckets, labels=False, duplicates='drop')\n",
    "\n",
    "                # Assign long and short positions based on buckets\n",
    "                long_positions = scores[buckets == (num_buckets - 1)]\n",
    "                short_positions = scores[buckets == 0]\n",
    "                long_positions['pos_type'] = 1\n",
    "                short_positions['pos_type'] = -1\n",
    "                positions = pd.concat([long_positions, short_positions])\n",
    "                positions['num_months'] = 1\n",
    "\n",
    "                # Load previous positions\n",
    "                if month == 1:\n",
    "                    prev_month = 12\n",
    "                    prev_yr = year-1\n",
    "                else:\n",
    "                    prev_month = month-1\n",
    "                    prev_yr = year\n",
    "\n",
    "                prev_month_yr = f\"{month_dict[prev_month]}{prev_yr}\"\n",
    "                port_path = f'{folder_path}\/portfolio_lead_lag\/{score_type}\/{prev_month_yr}.parquet'\n",
    "                prev_positions = pq.read_table(port_path).to_pandas()\n",
    "                prev_positions['num_months'] = prev_positions['num_months'] + 1\n",
    "                prev_positions = prev_positions[prev_positions['num_months'] <= 6]\n",
    "                print(f'Prev Holdings {len(prev_positions)}')\n",
    "\n",
    "                # Concatenate current and previous positions\n",
    "                positions = pd.concat([positions, prev_positions]).drop_duplicates(subset='cik')\n",
    "\n",
    "                # Separate long and short positions\n",
    "                long_positions = positions[positions['pos_type'] == 1]\n",
    "                short_positions = positions[positions['pos_type'] == -1]\n",
    "\n",
    "                # Calculate weights\n",
    "                long_positions['weight'] = 1\/len(long_positions)\n",
    "                short_positions['weight'] = -1\/len(short_positions)\n",
    "\n",
    "                # Concatenate long and short positions\n",
    "                positions = pd.concat([long_positions, short_positions])\n",
    "\n",
    "                # Assign the date\n",
    "                positions['date'] = month_yr\n",
    "                positions = positions[['date', 'cik', 'pos_type', 'num_months', 'weight', 'filing_date']]\n",
    "                positions = pd.merge(positions, cik_tick_map_req, how = 'inner', left_on = ['cik'], right_on = ['cik'])\n",
    "\n",
    "                # Save the positions to Parquet file\n",
    "                positions[['date', 'cik', 'pos_type', 'num_months', 'weight', 'ret', 'ret_clipped', 'filing_date', 'cshtrm']].to_parquet(\n",
    "                    f'{folder_path}\/portfolio_lead_lag\/{score_type}\/{month_yr}.parquet'\n",
    "                )\n",
    "                print(f'Num Holdings: {len(positions)}')"
   ],
   "execution_count":14,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "20\n",
      "1996-03-01\n",
      "1996-04-01\n",
      "Total rows: 10416\n",
      "Unique cusip count: 10217\n",
      "Total rows after selecting rows with max cshtrm: 10021\n",
      "18\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "<ipython-input-14-01f7ad8e37b7>:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_positions['pos_type'] = 1\n",
      "<ipython-input-14-01f7ad8e37b7>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_positions['pos_type'] = -1\n"
     ],
     "output_type":"stream"
    },
    {
     "ename":"FileNotFoundError",
     "evalue":"FileNotFoundError: \/data\/workspace_files\/parquet_files\/portfolios\/cosine_similarity\/Jan1996.parquet",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 51 in <module>",
      "    at line 1776 in read_table(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)",
      "    at line 1354 in __init__(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)",
      "    at line 782 in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)",
      "    at line 465 in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)",
      "    at line 441 in _ensure_single_source(path, filesystem)",
      "FileNotFoundError: \/data\/workspace_files\/parquet_files\/portfolios\/cosine_similarity\/Jan1996.parquet"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Sxdq0SD2TYGNHl8CkEyDyR",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "positions"
   ],
   "execution_count":44,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>date<\/th>\n",
       "      <th>cik<\/th>\n",
       "      <th>pos_type<\/th>\n",
       "      <th>num_months<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>ret<\/th>\n",
       "      <th>ret_clipped<\/th>\n",
       "      <th>filing_date<\/th>\n",
       "      <th>cshtrm<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>Jan1996<\/td>\n",
       "      <td>48465<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>3.0769<\/td>\n",
       "      <td>3.0769<\/td>\n",
       "      <td>1996-01-26<\/td>\n",
       "      <td>1090700.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Jan1996<\/td>\n",
       "      <td>107681<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>-1.0<\/td>\n",
       "      <td>-2.2776<\/td>\n",
       "      <td>-2.2776<\/td>\n",
       "      <td>1996-01-29<\/td>\n",
       "      <td>2544000.0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"xTJ9xISvOWKk3jcjdnv6Yz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"dvYlcGTvV9Wl7hVyqlPx1d",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "positions"
   ],
   "execution_count":16,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>date<\/th>\n",
       "      <th>cik<\/th>\n",
       "      <th>pos_type<\/th>\n",
       "      <th>num_months<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>filing_date<\/th>\n",
       "      <th>ret<\/th>\n",
       "      <th>ret_clipped<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>1069533<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.001091<\/td>\n",
       "      <td>2019-12-03<\/td>\n",
       "      <td>-10.7943<\/td>\n",
       "      <td>-10.7943<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>924717<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.001091<\/td>\n",
       "      <td>2019-12-03<\/td>\n",
       "      <td>-4.9240<\/td>\n",
       "      <td>-4.9240<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>1530425<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.001091<\/td>\n",
       "      <td>2019-12-03<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>1687926<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.001091<\/td>\n",
       "      <td>2019-12-05<\/td>\n",
       "      <td>3.0928<\/td>\n",
       "      <td>3.0928<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>876523<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.001091<\/td>\n",
       "      <td>2019-12-05<\/td>\n",
       "      <td>-8.7977<\/td>\n",
       "      <td>-8.7977<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1829<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>895419<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>3<\/td>\n",
       "      <td>-0.001082<\/td>\n",
       "      <td>2019-10-31<\/td>\n",
       "      <td>0.7367<\/td>\n",
       "      <td>0.7367<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1830<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>919956<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>3<\/td>\n",
       "      <td>-0.001082<\/td>\n",
       "      <td>2019-10-31<\/td>\n",
       "      <td>3.6364<\/td>\n",
       "      <td>3.6364<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1831<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>944314<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>3<\/td>\n",
       "      <td>-0.001082<\/td>\n",
       "      <td>2019-10-31<\/td>\n",
       "      <td>5.5135<\/td>\n",
       "      <td>5.5135<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1832<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>944695<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>3<\/td>\n",
       "      <td>-0.001082<\/td>\n",
       "      <td>2019-10-31<\/td>\n",
       "      <td>1.3975<\/td>\n",
       "      <td>1.3975<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1833<\/th>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>98362<\/td>\n",
       "      <td>-1<\/td>\n",
       "      <td>3<\/td>\n",
       "      <td>-0.001082<\/td>\n",
       "      <td>2019-10-31<\/td>\n",
       "      <td>-6.7128<\/td>\n",
       "      <td>-6.7128<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1834 rows × 8 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"dbce9CaIC36964FarMUfqr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "scores"
   ],
   "execution_count":13,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>cik<\/th>\n",
       "      <th>filing_date<\/th>\n",
       "      <th>cosine_similarity<\/th>\n",
       "      <th>jaccard_similarity<\/th>\n",
       "      <th>filing_monthyr<\/th>\n",
       "      <th>ret<\/th>\n",
       "      <th>ret_clipped<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>773717<\/td>\n",
       "      <td>2019-12-06<\/td>\n",
       "      <td>0.985736<\/td>\n",
       "      <td>0.889655<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-5.9508<\/td>\n",
       "      <td>-5.9508<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>773717<\/td>\n",
       "      <td>2019-12-23<\/td>\n",
       "      <td>0.983507<\/td>\n",
       "      <td>0.881145<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-5.9508<\/td>\n",
       "      <td>-5.9508<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>1067873<\/td>\n",
       "      <td>2019-12-16<\/td>\n",
       "      <td>0.996564<\/td>\n",
       "      <td>0.747925<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>1067873<\/td>\n",
       "      <td>2019-12-20<\/td>\n",
       "      <td>0.996909<\/td>\n",
       "      <td>0.764706<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>1067873<\/td>\n",
       "      <td>2019-12-23<\/td>\n",
       "      <td>0.996581<\/td>\n",
       "      <td>0.804622<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "      <td>-48.6116<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>363<\/th>\n",
       "      <td>1625376<\/td>\n",
       "      <td>2019-12-30<\/td>\n",
       "      <td>0.982843<\/td>\n",
       "      <td>0.695219<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>364<\/th>\n",
       "      <td>1681282<\/td>\n",
       "      <td>2019-12-30<\/td>\n",
       "      <td>0.976105<\/td>\n",
       "      <td>0.686209<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>-78.0000<\/td>\n",
       "      <td>-50.0000<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>365<\/th>\n",
       "      <td>1310630<\/td>\n",
       "      <td>2019-12-31<\/td>\n",
       "      <td>0.995134<\/td>\n",
       "      <td>0.882548<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "      <td>0.0000<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>366<\/th>\n",
       "      <td>1502152<\/td>\n",
       "      <td>2019-12-31<\/td>\n",
       "      <td>0.976843<\/td>\n",
       "      <td>0.725932<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>40.3226<\/td>\n",
       "      <td>40.3226<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>367<\/th>\n",
       "      <td>1502966<\/td>\n",
       "      <td>2019-12-31<\/td>\n",
       "      <td>0.998422<\/td>\n",
       "      <td>0.838459<\/td>\n",
       "      <td>Dec2019<\/td>\n",
       "      <td>2.0455<\/td>\n",
       "      <td>2.0455<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>368 rows × 7 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"UAgICdkxU2nw9RLPAJ7TgM",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "score_type = 'cosine_similarity'\n",
    "\n",
    "folder_path = '\/data\/workspace_files\/parquet_files'\n",
    "month_dict = { 1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "# Loop from March 2015 to March 2021\n",
    "for year in range(1997, 2020):\n",
    "    for month in range(1, 13):\n",
    "        month_yr = f'{month_dict[month]}{str(year)}'\n",
    "        long_positions = pq.read_table(f'{folder_path}\/portfolios\/{score_type}\/Quantile_5_{month_yr}.parquet').to_pandas()\n",
    "        long_positions['pos_type'] = 1\n",
    "        short_positions = pq.read_table(f'{folder_path}\/portfolios\/{score_type}\/Quantile_1_{month_yr}.parquet').to_pandas()\n",
    "        short_positions['pos_type'] = -1\n",
    "        short_positions['weight'] = -1*short_positions['weight']\n",
    "        positions = pd.concat([long_positions, short_positions])\n",
    "        positions.to_parquet(f'{folder_path}\/portfolios\/{month_yr}_v2.parquet')\n",
    "\n",
    "\n"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"80cX8C9wpGR6ABiDVQlaJN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_req_tick_cik_map(start_date, end_date):\n",
    "    # Assuming you have already imported the necessary libraries and connected to your database\n",
    "    # Assuming you have already imported the necessary libraries and connected to your database\n",
    "\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM crsp_a_stock.msf\n",
    "    WHERE ticker = 'AAPL'\n",
    "    \"\"\"\n",
    "\n",
    "    cik_tick_map = conn.raw_sql(sql_query)\n",
    "    return cik_tick_map\n",
    "    "
   ],
   "execution_count":24,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zKHjViVtb3ZHgk5CYBptQF",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "mcap, ret = get_req_tick_cik_map('2010-01-01', '2010-02-01')"
   ],
   "execution_count":34,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "2010-01-01\n",
      "2010-02-01\n"
     ],
     "output_type":"stream"
    },
    {
     "ename":"ProgrammingError",
     "evalue":"ProgrammingError: (psycopg2.errors.UndefinedColumn) column \"ipermno\" does not exist\nLINE 2:     SELECT distinct cik, tic, cusip, cshtrm, trt1m, ipermno\n                                                            ^\n\n[SQL: \n    SELECT distinct cik, tic, cusip, cshtrm, trt1m, ipermno\n    FROM comp_na_daily_all.secm\n    WHERE datadate >= '2010-01-01' and datadate <= '2010-02-01'\n    ]\n(Background on this error at: https:\/\/sqlalche.me\/e\/14\/f405)",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "ProgrammingError: (psycopg2.errors.UndefinedColumn) column \"ipermno\" does not exist\nLINE 2:     SELECT distinct cik, tic, cusip, cshtrm, trt1m, ipermno\n                                                            ^\n\n[SQL: \n    SELECT distinct cik, tic, cusip, cshtrm, trt1m, ipermno\n    FROM comp_na_daily_all.secm\n    WHERE datadate >= '2010-01-01' and datadate <= '2010-02-01'\n    ]\n(Background on this error at: https:\/\/sqlalche.me\/e\/14\/f405)"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"OUDPdu2yjNxlmCk7Utst9V",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "mcap.head()"
   ],
   "execution_count":17,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>cusip<\/th>\n",
       "      <th>permno<\/th>\n",
       "      <th>prc<\/th>\n",
       "      <th>shrout<\/th>\n",
       "      <th>cfacpr<\/th>\n",
       "      <th>cfacshr<\/th>\n",
       "      <th>Mcap<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>61745P86<\/td>\n",
       "      <td>86485.0<\/td>\n",
       "      <td>13.430000<\/td>\n",
       "      <td>17484.0<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>2.348101e+05<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>11133T10<\/td>\n",
       "      <td>91849.0<\/td>\n",
       "      <td>21.719999<\/td>\n",
       "      <td>136863.0<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>2.972664e+06<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>29273V10<\/td>\n",
       "      <td>91111.0<\/td>\n",
       "      <td>32.029999<\/td>\n",
       "      <td>222898.0<\/td>\n",
       "      <td>4.00<\/td>\n",
       "      <td>4.00<\/td>\n",
       "      <td>7.139423e+06<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>00211E10<\/td>\n",
       "      <td>93197.0<\/td>\n",
       "      <td>-2.530000<\/td>\n",
       "      <td>22770.0<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>1.00<\/td>\n",
       "      <td>-5.760810e+04<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>74347X17<\/td>\n",
       "      <td>92963.0<\/td>\n",
       "      <td>29.480000<\/td>\n",
       "      <td>150.0<\/td>\n",
       "      <td>0.25<\/td>\n",
       "      <td>0.25<\/td>\n",
       "      <td>4.422000e+03<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"yIZsXXtTicOQcLK8VXo6p9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "start_date = '2010-01-01'\n",
    "end_date = '2010-02-01'\n",
    "sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM comp_na_daily_all.secm\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "mcap = conn.raw_sql(sql_query)"
   ],
   "execution_count":20,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"LTqealw3jJyzgv1P7TPsDv",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "ret[ret['tic']=='MSFT']"
   ],
   "execution_count":29,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>cik<\/th>\n",
       "      <th>tic<\/th>\n",
       "      <th>cusip<\/th>\n",
       "      <th>cshtrm<\/th>\n",
       "      <th>ret<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6466<\/th>\n",
       "      <td>789019<\/td>\n",
       "      <td>MSFT<\/td>\n",
       "      <td>594918104<\/td>\n",
       "      <td>1.357981e+09<\/td>\n",
       "      <td>-7.5459<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"LQDxULDjbqXJuHIgZW2uM7",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "mcap[mcap['permno']==10107]"
   ],
   "execution_count":32,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>cusip<\/th>\n",
       "      <th>permno<\/th>\n",
       "      <th>prc<\/th>\n",
       "      <th>shrout<\/th>\n",
       "      <th>cfacpr<\/th>\n",
       "      <th>cfacshr<\/th>\n",
       "      <th>Mcap<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2602<\/th>\n",
       "      <td>59491810<\/td>\n",
       "      <td>10107.0<\/td>\n",
       "      <td>28.18<\/td>\n",
       "      <td>8770461.0<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>2.471516e+08<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"EDOWxPdQxPbArhkOdkrv0x",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"iRUqCHh8dAKB4AR1L5yMOd",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}